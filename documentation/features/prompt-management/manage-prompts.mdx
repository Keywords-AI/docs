---
title: "Advanced"
description: "Prompt schema, variables, composition, structured output, and deployment options for prompt templates."
---

<Accordion title="Set up Respan">
1. **Sign up** — Create an account at [platform.respan.ai](https://platform.respan.ai)
2. **Create an API key** — Generate one on the [API keys page](https://platform.respan.ai/platform/api/api-keys)
3. **Add credits or a provider key** — Add credits on the [Credits page](https://platform.respan.ai/platform/api/credits) or connect your own provider key on the [Integrations page](https://platform.respan.ai/platform/api/integrations)
</Accordion>


Build, test, and deploy prompt templates on Respan. New to prompts? Start with the [quickstart guide](/documentation/getting-started/quickstart/prompt_management).

---

## Prompt schema

The `prompt` object supports a `schema_version` field that controls how prompt configuration and request-body parameters are merged.

### Prompt schema v2 (recommended)

Set `schema_version=2` for the recommended merge behavior:

- Prompt configuration always wins for conflicting fields (no `override` flag needed).
- Uses prepend/instructions-style merging depending on the endpoint mode.
- Supports a `patch` field for applying additional parameter overrides. The `patch` object must **not** contain `messages` or `input`.

<Note>
**Important:** OpenAI SDKs strip fields like `schema_version`, `patch`, and `prompt_slug` during validation. Prompt schema v2 requires raw HTTP requests (e.g., `requests` in Python or `fetch` in TypeScript).
</Note>

<CodeGroup>
```python Python
import requests

headers = {
    'Content-Type': 'application/json',
    'Authorization': 'Bearer YOUR_RESPAN_API_KEY',
}

data = {
    'prompt': {
        'prompt_id': 'YOUR_PROMPT_ID',
        'schema_version': 2,
        'variables': {
            'task_description': 'Square a number',
        },
        'patch': {
            'temperature': 0.9,
            'max_tokens': 500
        }
    }
}

response = requests.post(
    'https://api.respan.ai/api/chat/completions',
    headers=headers,
    json=data
)
print(response.json())
```
```typescript TypeScript
fetch('https://api.respan.ai/api/chat/completions', {
    method: 'POST',
    headers: {
        'Content-Type': 'application/json',
        'Authorization': 'Bearer YOUR_RESPAN_API_KEY'
    },
    body: JSON.stringify({
        prompt: {
            prompt_id: 'YOUR_PROMPT_ID',
            schema_version: 2,
            variables: {
                task_description: 'Square a number',
            },
            patch: {
                temperature: 0.9,
                max_tokens: 500
            }
        }
    })
})
.then(response => response.json())
.then(data => console.log(data));
```
</CodeGroup>

### Prompt schema v1 (default, legacy)

When `schema_version` is absent or `1`, merging is controlled by the `override` flag:
- `override=true`: prompt configuration wins for all conflicting fields.
- `override=false` (default): request body wins for conflicting fields.

<Accordion title="Override other parameters">
```python
request_body = {
    "prompt": {
        "prompt_id": "042f5f",
        "override": True,
        "override_params": {
            "temperature": 0.8,
            "max_tokens": 150,
            "model": "gpt-4o"
        }
    }
}
```
</Accordion>

<Accordion title="Override prompt messages">
Append new messages to the end of existing prompt messages:
```python
request_body = {
    "prompt": {
        "prompt_id": "042f5f",
        "override_config": {"messages_override_mode": "append"},
        "override_params": {"messages": [{"role": "user", "content": "Additional context"}]},
    }
}
```

Replace all existing prompt messages:
```python
request_body = {
    "prompt": {
        "prompt_id": "042f5f",
        "override_config": {"messages_override_mode": "override"},
        "override_params": {"messages": [{"role": "user", "content": "Completely new conversation"}]},
    }
}
```
</Accordion>

---

## Variables

Use `{{variable_name}}` syntax to add dynamic content to your prompts. Variables let you reuse the same template across different inputs — pass values at runtime from code, or fill them from testset columns in experiments. See the [quickstart](/documentation/getting-started/quickstart/prompt_management) for setup and basic usage.

### Jinja templates

Respan supports [Jinja templates](https://jinja.palletsprojects.com/en/stable/templates/) for conditionals, filters, and JSON input access.

<Frame>
<img src="https://keywordsai-static.s3.us-east-1.amazonaws.com/docs/prompt/jinja-in-prompts.png" alt="Jinja support" />
</Frame>

| Feature | Syntax | Example |
|---------|--------|---------|
| Conditionals | `{% if %}...{% endif %}` | `{% if condition %}{{ variable_name }}{% endif %}` |
| JSON inputs | `{{ input.key }}` | `{{ input.name }}` |
| Filters | `{{ var \| filter }}` | `{{ variable_name \| filter_name }}` |
| Comments | `{# ... #}` | `{# This is a comment #}` |

See the [Filters in Jinja templates](/documentation/resources/cookbooks/filters-in-jinja) guide for details.

You can also set default variable values when creating a version via the API:

```python
data = {
    "messages": [
        {"role": "system", "content": "You are a helpful {{role}}."},
        {"role": "user", "content": "Help me with {{task_description}}."}
    ],
    "model": "gpt-4o-mini",
    "variables": {
        "role": "assistant",
        "task_description": "sorting an array"
    },
}
```

### Prompt composition

Prompt composition lets a variable in one prompt reference another prompt. At request time, the child is rendered first, converted to plain text, and inserted into the parent variable.

To use prompt composition, create two prompts: a **child** prompt and a **parent** prompt that has a `{{variable}}` where the child's output will be injected.

<Tabs>
<Tab title="Use in UI">

<Steps>
<Step title="Configure the variable">
In the parent prompt editor:
- Open the **Variables** panel on the right side
- Find the variable you want to embed a prompt into
- Change its type from **Text** to **Prompt**
- Select the child prompt
- Fill in all the child's variables

<Frame className="rounded-md">
<img src="https://keywordsai-static.s3.us-east-1.amazonaws.com/docs/documentation/products/prompt_management/advanced/prompt_composition.png" alt="Prompt composition setup" />
</Frame>
</Step>
<Step title="Run your prompt">
Click **Run** to test. The child prompt is rendered first, converted to plain text, and injected into the parent variable before the LLM sees it.
</Step>
</Steps>

**Limits:**
- Circular references are rejected (HTTP 400).
- Maximum prompt-chain depth is **2** (parent → child is safest). Exceeding this returns HTTP 400.

</Tab>
<Tab title="Use in code">

Instead of passing a plain string for a variable, pass a typed prompt object:

```json
{
  "_type": "prompt",
  "prompt_id": "CHILD_PROMPT_ID",
  "version": 1,
  "variables": {
    "some_key": "some_value"
  }
}
```

Supported fields:
- `_type` — must be `"prompt"`
- `prompt_id` — identifies the child prompt
- `version` — integer or `"latest"` (optional, defaults to deployed version)
- `variables` — nested variables for the child prompt (optional)

**Full example:**

```python
response = client.chat.completions.create(
    model="gpt-4o-mini",
    messages=[],
    extra_body={
        "prompt": {
            "prompt_id": "PARENT_PROMPT_ID",
            "override": True,
            "variables": {
                "conversation": {
                    "_type": "prompt",
                    "prompt_id": "CHILD_PROMPT_ID",
                    "version": 2,
                    "variables": {
                        "customer_name": "Sarah",
                        "department": "billing"
                    }
                },
                "request": "dispute a charge from last month"
            }
        }
    },
)
```

**Observability:** Resolved prompt variables are enriched in logs with `_rendered_result` — the plain-text output of the child prompt at runtime. This helps debug what the child actually resolved to.

See the [Chat Completions API reference](/apis/develop/gateway/chat-completions) for more details.

</Tab>
</Tabs>

---

## Structured output

Define structured output using JSON schema to ensure AI responses follow a specific format, following the [OpenAI Structured Outputs specification](https://platform.openai.com/docs/guides/structured-outputs?api-mode=responses).

<Tabs>
<Tab title="Setup">

Configure JSON schema in the **prompt editor** or the **playground**:

<Frame className="rounded-md">
<img src="https://keywordsai-static.s3.us-east-1.amazonaws.com/docs/documentation/products/prompt_management/creating_prompts/json_schema/setup1_prompt.png" alt="JSON schema in prompt editor" />
</Frame>

<Frame className="rounded-md">
<img src="https://keywordsai-static.s3.us-east-1.amazonaws.com/docs/documentation/products/prompt_management/creating_prompts/json_schema/setup2_playground.png" alt="JSON schema in playground" />
</Frame>

You can generate schemas using the AI generator or browse examples in the editor.

<Frame className="rounded-md">
<img src="https://keywordsai-static.s3.us-east-1.amazonaws.com/docs/documentation/products/prompt_management/creating_prompts/json_schema/generateai.png" alt="AI schema generator" />
</Frame>

</Tab>
<Tab title="Use in code">

Pass `response_format` when creating a prompt version:

<CodeGroup>
```python Python
data = {
    "messages": [
        {"role": "system", "content": "Extract the information."},
        {"role": "user", "content": "{{input_text}}"}
    ],
    "model": "gpt-4o-mini",
    "response_format": {
        "type": "json_schema",
        "json_schema": {
            "name": "extraction",
            "strict": True,
            "schema": {
                "type": "object",
                "properties": {
                    "date": {"type": "string"},
                    "customer_id": {"type": "integer"}
                },
                "required": ["date", "customer_id"],
                "additionalProperties": False
            }
        }
    }
}
```
```typescript TypeScript
const data = {
    messages: [
        {role: "system", content: "Extract the information."},
        {role: "user", content: "{{input_text}}"}
    ],
    model: "gpt-4o-mini",
    response_format: {
        type: "json_schema",
        json_schema: {
            name: "extraction",
            strict: true,
            schema: {
                type: "object",
                properties: {
                    date: {type: "string"},
                    customer_id: {type: "integer"}
                },
                required: ["date", "customer_id"],
                additionalProperties: false
            }
        }
    }
};
```
</CodeGroup>

</Tab>
</Tabs>

---

## Streaming

Enable streaming in the prompt settings sidebar. After enabling, **commit and deploy** the prompt.

<Frame>
  <img className="block dark:hidden" src="https://keywordsai-static.s3.us-east-1.amazonaws.com/docs/prompt/prompts-stream-light.jpg" alt="Streaming" />
  <img className="hidden dark:block" src="https://keywordsai-static.s3.us-east-1.amazonaws.com/docs/prompt/prompts-stream-dark.jpg" alt="Streaming" />
</Frame>

If you use a prompt with streaming enabled, you must also set `stream=True` in your SDK call:

<CodeGroup>
```python Python
response = client.chat.completions.create(
    model="gpt-4o-mini",
    messages=[{"role":"user", "content":"Tell me a long story"}],
    stream=True,
    extra_body={
      "prompt": {
          "prompt_id": "YOUR_PROMPT_ID",
          "variables": {"variable_name": "variable_value"},
        }
    }
)
```
```typescript TypeScript
const response = await client.chat.completions
  .create({
    messages: [{ role: "user", content: "Say this is a test" }],
    model: "gpt-4o-mini",
    stream: true,
    // @ts-expect-error
    prompt: {
      prompt_id: "YOUR_PROMPT_ID",
      variables: { variable_name: "variable_value" },
    },
  })
  .asResponse();

console.log(await response.json());
```
</CodeGroup>

---

## Deployment & versioning

<Tabs>
<Tab title="Via UI">

**Commit** saves a new version of your prompt. **Deploy** makes a version live for production traffic.

View version history in the **Overview** panel, or click **Version** in the Editor for detailed diffs.

<Frame>
<img src="https://keywordsai-static.s3.us-east-1.amazonaws.com/docs/prompt/prompts-activity-history.jpg" alt="Version history" />
</Frame>

Click on each version to see the diff of changes.

<Frame>
<img src="https://keywordsai-static.s3.us-east-1.amazonaws.com/docs/prompt/track-change.jpg" alt="Track changes" />
</Frame>

To deploy, go to the **Deployments** tab and click **Deploy**.

<Frame>
<img src="https://keywordsai-static.s3.us-east-1.amazonaws.com/docs/prompt/prompts-deploy.jpg" alt="Deploy prompt" />
</Frame>

To rollback, deploy an earlier version from the Deployments tab.

</Tab>
<Tab title="Via code">

### Version pinning

When calling a prompt from code, control which version is used:

- Omit `version` to use the **deployed** (live) version.
- Set `version` to a number (e.g., `3`) to **pin** that version.
- Use `"version": "latest"` to use the most recent **draft** (not deployed).

```python
{
    "prompt": {
        "prompt_id": "YOUR_PROMPT_ID",
        "version": 3,
    }
}
```

### Deploy via API

Use the [Prompt Versions API](/apis/develop/prompts/create-prompt-version) to create and deploy versions programmatically. Set `deploy: true` to make a version live immediately.

<CodeGroup>
```python Python
import requests

url = "https://api.respan.ai/api/prompts/<prompt_id>/versions/"
headers = {
    "Authorization": "Bearer YOUR_RESPAN_API_KEY",
    "Content-Type": "application/json"
}

data = {
    "messages": [
        {"role": "system", "content": "You are a helpful {{role}}."},
        {"role": "user", "content": "Hello, how are you?"}
    ],
    "model": "gpt-4o-mini",
    "temperature": 0.7,
    "max_tokens": 256,
    "deploy": True  # Deploy as the live version immediately
}

response = requests.post(url, headers=headers, json=data)
print(response.json())
```
```typescript TypeScript
fetch('https://api.respan.ai/api/prompts/<prompt_id>/versions/', {
    method: 'POST',
    headers: {
        'Content-Type': 'application/json',
        'Authorization': 'Bearer YOUR_RESPAN_API_KEY'
    },
    body: JSON.stringify({
        messages: [
            { role: "system", content: "You are a helpful {{role}}." },
            { role: "user", content: "Hello, how are you?" }
        ],
        model: "gpt-4o-mini",
        temperature: 0.7,
        max_tokens: 256,
        deploy: true  // Deploy as the live version immediately
    })
})
.then(response => response.json())
.then(data => console.log(data));
```
</CodeGroup>

To deploy an existing version, use the [Update Version API](/apis/develop/prompts/update-prompt-version) with `deploy: true`:

```python
url = "https://api.respan.ai/api/prompts/<prompt_id>/versions/<version_id>/"
data = {"deploy": True}

response = requests.patch(url, headers=headers, json=data)
```

</Tab>
</Tabs>

---

## Playground

Test and iterate on prompts in the [Prompt Playground](https://platform.respan.ai/platform/playground).

<Frame>
<video controls className="w-full aspect-video" src="https://keywordsai-static.s3.us-east-1.amazonaws.com/docs/prompt/playground.mp4"></video>
</Frame>

From the prompt editor, enter variable values and click **Playground** in the top bar to test. Click **Commit** to save changes back to your prompt library.

<Frame>
<img src="https://keywordsai-static.s3.us-east-1.amazonaws.com/docs/prompt/prompt-to-playground.png" alt="Bring prompt to playground" />
</Frame>

You can also debug production logs by clicking **Open in Playground** on any log entry.

<Frame>
<img src="https://keywordsai-static.s3.us-east-1.amazonaws.com/docs/prompt/logs-to-playground.png" alt="Debug from logs" />
</Frame>

Set the number of variants in the side panel to generate multiple responses and compare them in the **Variants** tab.

<Frame>
<img src="https://keywordsai-static.s3.us-east-1.amazonaws.com/docs/prompt/playground-set-variants.jpg" alt="Set variants" />
</Frame>

---

## Prompt logging

<Tabs>
<Tab title="Respan prompts">

Log prompt usage to track performance metrics, compare versions, and analyze request distribution.

<CodeGroup>
```python Python
import requests

url = "https://api.respan.ai/api/request-logs/create/"
payload = {
    "model": "claude-3-5-sonnet-20240620",
    "completion_message": {
        "role": "assistant",
        "content": "Hi, how can I assist you today?"
    },
    "prompt": {
        "prompt_id": "xxxxxx",
        "variables": {
            "task_description": "Square a number",
            "specific_library": "math"
        },
    },
    "generation_time": 5.7,
    "ttft": 3.1,
}
headers = {
    "Authorization": "Bearer YOUR_RESPAN_API_KEY",
    "Content-Type": "application/json"
}

response = requests.post(url, headers=headers, json=payload)
```
```typescript TypeScript
const url = 'https://api.respan.ai/api/request-logs/create/';
const headers = {
    'Authorization': 'Bearer YOUR_RESPAN_API_KEY',
    'Content-Type': 'application/json'
};

const payload = {
    model: 'claude-3-5-sonnet-20240620',
    completion_message: {
        role: "assistant",
        content: "Hi, how can I assist you today?"
    },
    prompt: {
        prompt_id: "xxxxxx",
        variables: {
            task_description: "Square a number",
            specific_library: "math"
        },
    },
    generation_time: 5.7,
    ttft: 3.1,
};

fetch(url, {
    method: 'POST',
    headers: headers,
    body: JSON.stringify(payload)
})
.then(response => response.json())
.then(data => console.log(data));
```
</CodeGroup>

<Frame>
<video controls className="w-full aspect-video" src="https://keywordsai-static.s3.us-east-1.amazonaws.com/docs/prompt/prompt-monitor.mp4"></video>
</Frame>

Filter logs by prompt name on the [Logs page](https://platform.respan.ai/platform/logs).

<Frame>
<img src="https://keywordsai-static.s3.us-east-1.amazonaws.com/docs/prompt/prompt-logs.png" alt="Prompt logs" />
</Frame>

</Tab>
<Tab title="External prompts">

If you manage prompts outside of Respan, pass any `prompt_id` and set `is_custom_prompt: true`:

<CodeGroup>
```python Python
payload = {
    "model": "claude-3-5-sonnet-20240620",
    "prompt_messages": [{"role": "user", "content": "Hi"}],
    "completion_message": {"role": "assistant", "content": "Hello!"},
    "prompt_id": "your-custom-id",
    "is_custom_prompt": True,
    "generation_time": 5.7
}
```
```typescript TypeScript
const payload = {
    model: 'claude-3-5-sonnet-20240620',
    prompt_messages: [{role: 'user', content: 'Hi'}],
    completion_message: {role: 'assistant', content: 'Hello!'},
    prompt_id: 'your-custom-id',
    is_custom_prompt: true,
    generation_time: 5.7
};
```
</CodeGroup>

</Tab>
</Tabs>

---

## Team collaboration

- **Share** — click the **Link** button in the Editor to share a prompt
- **Comments** — add comments to discuss changes
- **Labels** — categorize and organize prompts

<Frame>
<img src="https://keywordsai-static.s3.us-east-1.amazonaws.com/docs/prompt/prompt-share.png" alt="Share prompt" />
</Frame>

---

## Parameters reference

<ParamField path="prompt_id" type="string" required>
  The unique identifier of your saved prompt template.
</ParamField>

<ParamField path="variables" type="object">
  Variables to inject into your prompt template. Values can be strings or typed prompt objects for [composition](#prompt-composition).
```json
{
  "variables": {
    "user_name": "John",
    "task": "summarize"
  }
}
```
</ParamField>

<ParamField path="override" type="boolean" default={false}>
  When `true`, the saved prompt configuration overrides SDK parameters like `model` and `messages`.
</ParamField>

<ParamField path="override_params" type="object">
  Parameters that override your saved prompt configuration (temperature, max_tokens, messages, model, etc.).
</ParamField>

<ParamField path="override_config" type="object">
  Controls how override parameters are applied.
  - `messages_override_mode`: `"append"` (add to existing) or `"override"` (replace all)
</ParamField>

<ParamField path="schema_version" type="integer" default={1}>
  Controls the prompt merge strategy. `1` (default, legacy) uses `override` flag logic. `2` (recommended) uses prepend/instructions-style merging where the prompt config always wins. See [Prompt schema](#prompt-schema).
</ParamField>

<ParamField path="patch" type="object">
  Additional parameter overrides applied in v2 mode (`schema_version=2`). Must **not** contain `messages` or `input`. Useful for overriding fields like `temperature` or `max_tokens` while letting the prompt config control messages and model.
</ParamField>

<ParamField path="echo" type="boolean" default={false}>
  When enabled, the response includes the final prompt messages used.
</ParamField>

<ParamField path="version" type="integer | string">
  Pin a specific prompt version. Omit for deployed version, use `"latest"` for newest draft.
</ParamField>

<Info>See [all Respan supported params](/apis/develop/gateway/chat-completions#respan-parameters).</Info>
