---
title: "Metrics"
description: "A guide to view LLM usage metrics and user analytics"
og:title: "AI observability"
---
<Accordion title="Set up Respan">
1. **Sign up** — Create an account at [platform.respan.ai](https://platform.respan.ai)
2. **Create an API key** — Generate one on the [API keys page](https://platform.respan.ai/platform/api/api-keys)
3. **Add credits or a provider key** — Add credits on the [Credits page](https://platform.respan.ai/platform/api/credits) or connect your own provider key on the [Integrations page](https://platform.respan.ai/platform/api/integrations)
</Accordion>

<Accordion title="Use AI">
Add the [Docs MCP](/documentation/getting-started/ask-ai) to your AI coding tool to get help building with Respan. No API key needed.
```json
{
  "mcpServers": {
    "respan-docs": {
      "url": "https://docs.respan.ai/mcp"
    }
  }
}
```
</Accordion>



## Why observability matters

**Performance monitoring** tracks response times and model performance to ensure optimal operation.

**Cost management** identifies expensive prompts and optimizes spending across LLM providers.

**Quality assurance** detects issues and unexpected outputs before they reach users.

**Debugging** enables quick problem identification through complete session examination.

Without proper observability, LLM applications become expensive black boxes that are impossible to systematically improve.

<Frame className="rounded-md">
    <img src="https://keywordsai-static.s3.us-east-1.amazonaws.com/docs/documentation/products/dashboard/overview/dashboard.png" />
</Frame>

## What are LLM usage metrics?

LLM usage metrics provide comprehensive monitoring for your AI applications. Track key indicators like total requests, token usage, errors, latency, and costs. 

Break down analytics by model, user, API key, and prompt for complete visibility into your operations.

---
## Need help?
[Join our discord](https://discord.com/invite/KEanfAafQQ) — we'll help you pick the best fit.