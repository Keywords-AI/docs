---
title: "Evaluators"
description: "Set up LLM, code, and human evaluators to score your AI outputs."
---
<Accordion title="Set up Respan">
1. **Sign up** — Create an account at [platform.respan.ai](https://platform.respan.ai)
2. **Create an API key** — Generate one on the [API keys page](https://platform.respan.ai/platform/api/api-keys)
3. **Add credits or a provider key** — Add credits on the [Credits page](https://platform.respan.ai/platform/api/credits) or connect your own provider key on the [Integrations page](https://platform.respan.ai/platform/api/integrations)
</Accordion>

<Accordion title="Use AI">
Add the [Docs MCP](/documentation/getting-started/ask-ai) to your AI coding tool to get help building with Respan. No API key needed.
```json
{
  "mcpServers": {
    "respan-docs": {
      "url": "https://docs.respan.ai/mcp"
    }
  }
}
```
</Accordion>



Evaluators score your LLM outputs — automatically with an LLM judge, programmatically with code, or manually with human review. Create them on Respan, then trigger from code, the gateway, or experiments.

<Note>This is a beta feature. The [API documentation](/apis/evaluate/evaluators/create) is the source of truth for evaluator configuration and behavior.</Note>

---

## Set up an evaluator

Go to [Evaluators](https://platform.respan.ai/platform/evaluators) and click **+ New evaluator**. Select the evaluator type:

<Tabs>
<Tab title="LLM evaluator">

LLM evaluators use a language model to score outputs automatically.

<Steps>

<Step title="Configure the evaluator">
Define a **Slug** — a unique identifier used to reference this evaluator in API calls and logs.

<Warning>
Don't change the slug after creation. It is used to identify the evaluator across your logs and code.
</Warning>

Choose a **model** for the evaluator. Currently supported: `gpt-4o` and `gpt-4o-mini` (OpenAI and Azure OpenAI).

<Frame>
<img src="https://keywordsai-static.s3.us-east-1.amazonaws.com/docs/documentation/products/evaluators/evaluator_v0.png"/>
</Frame>
</Step>

<Step title="Write the definition">
The definition is the core instruction that tells the LLM how to evaluate. You can use these variables:

| Variable | Description |
|----------|-------------|
| `{{input}}` | The input prompt sent to the LLM |
| `{{output}}` | The response generated by the LLM |
| `{{metadata}}` | Custom metadata associated with the request |
| `{{metrics}}` | System-captured metrics (latency, tokens, etc.) |

<Note>
**Ideal output**: `ideal_output` is not a standalone variable. To compare against a reference answer, include it in your `metadata` and reference it as `{{metadata.ideal_output}}`.
</Note>
</Step>

<Step title="Define the scoring rubric">
Set the scoring rubric to guide how the LLM assigns scores. Set a **passing score** (minimum score for a response to pass).

<Frame>
<img src="https://keywordsai-static.s3.us-east-1.amazonaws.com/docs/evaluations/llm-evals/define-scores.png"/>
</Frame>

Click **Save** to create the evaluator.
</Step>

</Steps>

</Tab>
<Tab title="Code evaluator">

Code evaluators run custom logic to score outputs programmatically. Write a Python function that takes the unified input and returns a score.

<Steps>

<Step title="Configure the evaluator">
Define a **Slug** and select **Code** as the evaluator type.
</Step>

<Step title="Write the evaluation function">
Write a Python function that receives the unified input object and returns a score. The function has access to:

- `input` — the request content
- `output` — the response content
- `metadata` — custom metadata
- `metrics` — performance data (latency, cost, etc.)
</Step>

<Step title="Define the scoring rubric">
Set the score range and passing threshold, then click **Save**.
</Step>

</Steps>

</Tab>
<Tab title="Human evaluator">

Human evaluators let your team manually score LLM outputs for quality assurance.

<Steps>

<Step title="Configure the evaluator">
Name the evaluator and add a description. Choose the metric type: **Boolean** or **Numerical**.

<Frame>
<img src="https://keywordsai-static.s3.us-east-1.amazonaws.com/docs/evaluations/human-evals/config.png"/>
</Frame>

Click **Save** to create the evaluator.
</Step>

<Step title="Annotate logs">
Open any log in [Logs](https://platform.respan.ai/platform/logs) and score it in the **Scores** section of the side panel.

<Frame>
<img src="https://keywordsai-static.s3.us-east-1.amazonaws.com/docs/evaluations/human-evals/annotation.png"/>
</Frame>
</Step>

</Steps>

</Tab>
</Tabs>
