---
title: LiteLLM
description: "Use LiteLLM with Respan for gateway access and logging."
---

<Accordion title="Set up Respan">
1. **Sign up** — Create an account at [platform.respan.ai](https://platform.respan.ai)
2. **Create an API key** — Generate one on the [API keys page](https://platform.respan.ai/platform/api/api-keys)
3. **Add credits or a provider key** — Add credits on the [Credits page](https://platform.respan.ai/platform/api/credits) or connect your own provider key on the [Integrations page](https://platform.respan.ai/platform/api/integrations)
</Accordion>

<Note> This integration supports the **Respan gateway** and **callback logging**. </Note>

## What is LiteLLM?

[LiteLLM](https://www.litellm.ai/) provides a unified Python interface for calling 100+ LLM providers using the OpenAI format. With Respan, you can either:
- **Gateway** — route requests through Respan for observability, fallbacks, and load balancing
- **Logging** — keep calling providers directly and export logs to Respan via a callback

## Quickstart

### Install packages

```bash
pip install litellm respan-exporter-litellm
```

### Gateway mode

Route LiteLLM requests through the Respan gateway. This gives you full access to Respan features like fallbacks, load balancing, and prompt management.

```python
import litellm

response = litellm.completion(
    api_key="YOUR_RESPAN_API_KEY",
    api_base="https://api.respan.ai/api",
    model="gpt-4o-mini",
    messages=[{"role": "user", "content": "Hello!"}],
)
print(response.choices[0].message.content)
```

<Frame className="rounded-md">
<img src="https://keywordsai-static.s3.us-east-1.amazonaws.com/docs/Integrations/LiteLLM/proxy_log.png" alt="LiteLLM gateway logging in Respan" />
</Frame>

### Logging mode

Register the Respan callback to log all completions automatically. Requests go directly to providers — only the logs are sent to Respan.

```python
import litellm
from respan_exporter_litellm import RespanLiteLLMCallback

litellm.callbacks = [RespanLiteLLMCallback()]

response = litellm.completion(
    model="gpt-4o-mini",
    messages=[{"role": "user", "content": "Hello!"}],
)
print(response.choices[0].message.content)
```

<Frame className="rounded-md">
<img src="https://keywordsai-static.s3.us-east-1.amazonaws.com/docs/Integrations/LiteLLM/callback_log.png" alt="LiteLLM callback logging in Respan" />
</Frame>

## Supported parameters

### Gateway mode (extra_body)

Pass Respan parameters using `extra_body` when routing through the gateway.

```python
response = litellm.completion(
    api_key="YOUR_RESPAN_API_KEY",
    api_base="https://api.respan.ai/api",
    model="gpt-4o-mini",
    messages=[{"role": "user", "content": "Hello!"}],
    extra_body={
        "customer_identifier": "user-123",
        "metadata": {"session_id": "abc123"},
        "thread_identifier": "conversation_456",
    },
)
```

### Logging mode (metadata)

Pass Respan parameters inside `metadata.respan_params`.

```python
response = litellm.completion(
    model="gpt-4o-mini",
    messages=[{"role": "user", "content": "Hello!"}],
    metadata={
        "respan_params": {
            "workflow_name": "simple_logging",
            "span_name": "single_log",
            "customer_identifier": "user-123",
        }
    },
)
```

## Configuration

| Parameter | Type | Default | Description |
|-----------|------|---------|-------------|
| `api_key` | `str` | `RESPAN_API_KEY` env var | Respan API key. |
| `base_url` | `str \| None` | `None` | API base URL. |

See the [LiteLLM Exporter SDK reference](/sdks/python/exporters/litellm) for the full API.

## Async usage

The callback supports async completions automatically:

```python
import litellm
from respan_exporter_litellm import RespanLiteLLMCallback

litellm.callbacks = [RespanLiteLLMCallback()]

response = await litellm.acompletion(
    model="gpt-4o-mini",
    messages=[{"role": "user", "content": "Tell me a joke"}],
)
```

## Multiple providers

LiteLLM's unified interface means all providers are logged with the same callback:

```python
import litellm
from respan_exporter_litellm import RespanLiteLLMCallback

litellm.callbacks = [RespanLiteLLMCallback()]

# OpenAI
litellm.completion(model="gpt-4o-mini", messages=[...])

# Anthropic
litellm.completion(model="claude-sonnet-4-5-20250929", messages=[...])

# Together
litellm.completion(model="together_ai/meta-llama/Llama-3-70b", messages=[...])
```

## Next Steps

<CardGroup cols={2}>
<Card title="User Management" href="/documentation/features/user-analytics/customer-identifier">
  Track user behavior and patterns
</Card>
<Card title="Prompt Management" href="/documentation/features/prompt-management/quickstart">
  Manage and version your prompts
</Card>
</CardGroup>
