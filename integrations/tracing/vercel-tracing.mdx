---
title: "Vercel AI SDK"
description: "Trace AI SDK (Vercel) calls with Respan."
---

<Accordion title="Set up Respan">
1. **Sign up** — Create an account at [platform.respan.ai](https://platform.respan.ai)
2. **Create an API key** — Generate one on the [API keys page](https://platform.respan.ai/platform/api/api-keys)
3. **Add credits or a provider key** — Add credits on the [Credits page](https://platform.respan.ai/platform/api/credits) or connect your own provider key on the [Integrations page](https://platform.respan.ai/platform/api/integrations)
</Accordion>

<Accordion title="Use AI">
Add the [Docs MCP](/documentation/getting-started/ask-ai) to your AI coding tool to get help building with Respan. No API key needed.
```json
{
  "mcpServers": {
    "respan-docs": {
      "url": "https://docs.respan.ai/mcp"
    }
  }
}
```
</Accordion>

## What is AI SDK?

The [AI SDK](https://ai-sdk.dev/) (by Vercel) is a TypeScript toolkit for building AI-powered applications with Next.js, React, and other frameworks. It provides unified APIs for text generation, streaming, tool use, and structured outputs across multiple LLM providers.

<Check>
Example project: [GitHub Link](https://github.com/respanai/respan-example-projects/tree/main/typescript/tracing/vercel-tracing)
</Check>

## Setup

<Steps>
<Step title="Install packages">

```bash
npm install @respan/exporter-vercel @vercel/otel ai @ai-sdk/openai
```

</Step>

<Step title="Set environment variables">

```bash
export RESPAN_API_KEY="YOUR_RESPAN_API_KEY"
export OPENAI_API_KEY="YOUR_OPENAI_API_KEY"
```

</Step>

<Step title="Create instrumentation file">

Create `instrumentation.ts` in your project root (Next.js) or entry point:

```typescript
import { registerOTel } from '@vercel/otel';
import { RespanExporter } from '@respan/exporter-vercel';

export function register() {
  registerOTel({
    serviceName: 'my-app',
    traceExporter: new RespanExporter({
      apiKey: process.env.RESPAN_API_KEY!,
    }),
  });
}
```

</Step>

<Step title="Enable telemetry in AI calls">

```typescript
import { generateText } from 'ai';
import { openai } from '@ai-sdk/openai';

const result = await generateText({
  model: openai('gpt-4o-mini'),
  prompt: 'Tell me a joke about AI',
  experimental_telemetry: { isEnabled: true },
});
console.log(result.text);
```

<Accordion title="Using Vercel AI Gateway?">
If you're already routing requests through the [Vercel AI Gateway](https://vercel.com/docs/ai-gateway), no extra configuration is needed — the `RespanExporter` from Step 3 automatically exports all traced spans, including those routed through the Vercel AI Gateway.

Just make sure `experimental_telemetry: { isEnabled: true }` is set on your AI calls.
</Accordion>

</Step>

<Step title="View your trace">

Open the [Traces page](https://platform.respan.ai/platform/traces) to see your AI calls.

</Step>
</Steps>

## Configuration

| Parameter | Type | Default | Description |
|-----------|------|---------|-------------|
| `apiKey` | `string` | `RESPAN_API_KEY` env var | Respan API key. |
| `baseUrl` | `string` | `"https://api.respan.ai"` | API base URL. |
| `debug` | `boolean` | `false` | Enable debug logging for troubleshooting. |

See the [Vercel Exporter SDK reference](/sdks/typescript/exporters/vercel) for the full API.

<Note>
**Next.js streaming routes**: If your AI calls use streaming (`streamText`), set `maxDuration` in your route handler to avoid Vercel's default timeout:

```typescript
// app/api/chat/route.ts
export const maxDuration = 30;
```
</Note>

## Attributes

Pass metadata and customer identifiers through the telemetry config or request headers.

### Via telemetry metadata

```typescript
const result = await generateText({
  model: openai('gpt-4o-mini'),
  prompt: 'Hello',
  experimental_telemetry: {
    isEnabled: true,
    metadata: {
      customer_identifier: 'user-123',
      thread_identifier: 'thread-abc',
    },
  },
});
```

### Via request headers

```typescript
const result = await generateText({
  model: openai('gpt-4o-mini'),
  prompt: 'Hello',
  experimental_telemetry: { isEnabled: true },
  headers: {
    'X-Respan-Customer-Identifier': 'user-123',
  },
});
```

| Attribute | Description |
|-----------|-------------|
| `customer_identifier` | User/customer identifier for filtering. |
| `thread_identifier` | Conversation thread ID. |

## Troubleshooting

<AccordionGroup>
<Accordion title="Missing @vercel/otel dependencies">
The `@vercel/otel` package may have broken peer dependencies. If you see missing module errors on startup, install them directly:

```bash
npm install @opentelemetry/api-logs
```

If this fails, pin your package manager version in `package.json`:

```json
{
  "packageManager": "yarn@4.9.2"
}
```
</Accordion>

<Accordion title="No traces appearing">
1. Verify `experimental_telemetry: { isEnabled: true }` is set on every AI SDK call
2. Check that `instrumentation.ts` is in your project root (same level as `package.json`)
3. Enable debug mode to see exporter logs:

```typescript
new RespanExporter({
  apiKey: process.env.RESPAN_API_KEY!,
  debug: true,
})
```
</Accordion>
</AccordionGroup>

<Note>
Looking for gateway integration? See [Gateway > Vercel AI SDK](/integrations/gateway/vercel).
</Note>
